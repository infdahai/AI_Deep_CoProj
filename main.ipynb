{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import re\n",
    "\n",
    "import string\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import random\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6375 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 2428\n",
      "Id to word mapping, for example: 2428 -> flower\n",
      "Tokens: <PAD>: 6372; <RARE>: 6374\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path+'/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path+'/word2Id.npy'))\n",
    "id2word_dict =  dict(np.load(dictionary_path+'/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s'%('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s'%('2428', id2word_dict['2428']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s'%(word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['2435', '2428', '2505', '2431', '2437', '2465', '2446', '2457', '2429', '2455', '2446', '6374', '6372', '6372', '6372', '6372', '6372', '6372', '6372', '6372']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    tokens = []\n",
    "    tokens.extend(nltk.tokenize.word_tokenize(prep_line.lower()))\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [word2Id_dict[tokens[k]] if tokens[k] in word2Id_dict else word2Id_dict['<RARE>'] for k in range(len(tokens))]\n",
    "    \n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path+'/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data'%(n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>[[2430, 2428, 2431, 2427, 2436, 2432, 2450, 24...</td>\n",
       "      <td>/102flowers/image_08110.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>[[2430, 2428, 2431, 2427, 2436, 2432, 2440, 24...</td>\n",
       "      <td>/102flowers/image_07749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7908</th>\n",
       "      <td>[[2435, 2428, 2505, 2431, 2444, 2427, 2433, 24...</td>\n",
       "      <td>/102flowers/image_04381.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>[[2430, 2428, 2431, 2563, 2437, 2427, 2433, 24...</td>\n",
       "      <td>/102flowers/image_04518.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>[[2435, 2428, 2427, 2432, 5409, 2429, 2432, 24...</td>\n",
       "      <td>/102flowers/image_07620.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "1855  [[2430, 2428, 2431, 2427, 2436, 2432, 2450, 24...   \n",
       "6790  [[2430, 2428, 2431, 2427, 2436, 2432, 2440, 24...   \n",
       "7908  [[2435, 2428, 2505, 2431, 2444, 2427, 2433, 24...   \n",
       "1805  [[2430, 2428, 2431, 2563, 2437, 2427, 2433, 24...   \n",
       "5679  [[2435, 2428, 2427, 2432, 5409, 2429, 2432, 24...   \n",
       "\n",
       "                        ImagePath  \n",
       "1855  /102flowers/image_08110.jpg  \n",
       "6790  /102flowers/image_07749.jpg  \n",
       "7908  /102flowers/image_04381.jpg  \n",
       "1805  /102flowers/image_04518.jpg  \n",
       "5679  /102flowers/image_07620.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_DEPTH = 3\n",
    "def training_data_generator(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    imagefile = tf.read_file(image_path)\n",
    "    image = tf.image.decode_image(imagefile, channels=3)\n",
    "    float_img = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    float_img.set_shape([None, None, 3])\n",
    "    image = tf.image.resize_images(float_img, size = [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "    \n",
    "    return image, caption\n",
    "\n",
    "def data_iterator(filenames, batch_size, data_generator):\n",
    "    # Load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i])) \n",
    "    caption = np.asarray(caption)\n",
    "    image_path = df['ImagePath'].values\n",
    "\n",
    "    # Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    \n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "BATCH_SIZE = 64\n",
    "iterator_train, types, shapes = data_iterator(data_path+'/text2ImgData.pkl', BATCH_SIZE, training_data_generator)\n",
    "iter_initializer = iterator_train.initializer\n",
    "next_element = iterator_train.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator_train.initializer)\n",
    "    next_element = iterator_train.get_next()\n",
    "    image, text = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text (a list of id)\n",
    "    output: hidden representation of input text in dimention of TEXT_DIM\n",
    "    \"\"\"\n",
    "    def __init__(self, text, hparas, training_phase=True, reuse=False, return_embed=False):\n",
    "        self.text = text\n",
    "        self.hparas = hparas\n",
    "        self.train = training_phase\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('rnnftxt', reuse=self.reuse):\n",
    "            # Word embedding\n",
    "            word_embed_matrix = tf.get_variable('rnn/wordembed', \n",
    "                                                shape=(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM']),\n",
    "                                                initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                                                dtype=tf.float32)\n",
    "            embedded_word_ids = tf.nn.embedding_lookup(word_embed_matrix, self.text)\n",
    "            # RNN encoder\n",
    "            LSTMCell = tf.nn.rnn_cell.LSTMCell(self.hparas['TEXT_DIM'], reuse=self.reuse)\n",
    "            initial_state = LSTMCell.zero_state(self.hparas['BATCH_SIZE'], dtype=tf.float32)\n",
    "            rnn_net = tf.nn.dynamic_rnn(cell=LSTMCell, \n",
    "                                        inputs=embedded_word_ids, \n",
    "                                        initial_state=initial_state, \n",
    "                                        dtype=np.float32, time_major=False,\n",
    "                                        scope='rnn/dynamic')\n",
    "            self.rnn_net = rnn_net\n",
    "            self.outputs = rnn_net[0][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, noise_z, text, training_phase, hparas, reuse):\n",
    "        self.z = noise_z\n",
    "        self.text = text\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.gf_dim = 128\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('generator', reuse=self.reuse):\n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(text_flatten, self.hparas['TEXT_DIM'], name='generator/text_input', reuse=self.reuse)\n",
    "            z_text_concat = tf.concat([self.z, text_input], axis=1, name='generator/z_text_concat')\n",
    "            g_net = tf.layers.dense(z_text_concat, 64*64*3, name='generator/g_net', reuse=self.reuse)\n",
    "            g_net = tf.reshape(g_net, [-1, 64, 64, 3], name='generator/g_net_reshape')\n",
    "            \n",
    "            self.generator_net = g_net\n",
    "            self.outputs = g_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet structure\n",
    "class Discriminator:\n",
    "    def __init__(self, image, text, training_phase, hparas, reuse):\n",
    "        self.image = image\n",
    "        self.text = text\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.df_dim = 128 # 196 for MSCOCO\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):        \n",
    "        with tf.variable_scope('discriminator', reuse=self.reuse):\n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(text_flatten, self.hparas['TEXT_DIM'], name='discrim/text_input', reuse=self.reuse)\n",
    "            image_flatten = tf.contrib.layers.flatten(self.image)\n",
    "            image_input = tf.layers.dense(image_flatten, self.hparas['TEXT_DIM'], name='discrim/image_input', reuse=self.reuse)\n",
    "            img_text_concate = tf.concat([text_input, image_input], axis=1, name='discrim/concate')\n",
    "            d_net = tf.layers.dense(img_text_concate, 1, name='discrim/d_net', reuse=self.reuse)\n",
    "            \n",
    "            self.logits = d_net\n",
    "            net_output = tf.nn.sigmoid(d_net)\n",
    "            self.discriminator_net = net_output\n",
    "            self.outputs = net_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparas():\n",
    "    hparas = {\n",
    "        'MAX_SEQ_LENGTH' : 20,\n",
    "        'EMBED_DIM' : 16, # word embedding dimension\n",
    "        'VOCAB_SIZE' : len(vocab),\n",
    "        'TEXT_DIM' : 16, # text embedding dimension\n",
    "        'RNN_HIDDEN_SIZE' : 16,\n",
    "        'Z_DIM' : 16, # random noise z dimension\n",
    "        'IMAGE_SIZE' : [64, 64, 3], # render image size\n",
    "        'BATCH_SIZE' : 64,\n",
    "        'LR' : 0.01,\n",
    "        'BETA' : 0.5, # AdamOptimizer parameter\n",
    "        'N_EPOCH' : 5,\n",
    "        'N_SAMPLE' : num_training_sample\n",
    "    }\n",
    "    return hparas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, hparas, training_phase, dataset_path, ckpt_path, inference_path, recover=None):\n",
    "        self.hparas = hparas\n",
    "        self.train = training_phase\n",
    "        self.dataset_path = dataset_path # dataPath+'/text2ImgData.pkl'\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.sample_path = './samples'\n",
    "        self.inference_path = './inference'\n",
    "        \n",
    "        self._get_session() # get session\n",
    "        self._get_train_data_iter() # initialize and get data iterator\n",
    "        self._input_layer() # define input placeholder\n",
    "        self._get_inference() # build generator and discriminator\n",
    "        self._get_loss() # define gan loss\n",
    "        self._get_var_with_name() # get variables for each part of model\n",
    "        self._optimize() # define optimizer\n",
    "        self._init_vars()\n",
    "        self._get_saver()\n",
    "        \n",
    "        if recover is not None:\n",
    "            self._load_checkpoint(recover)\n",
    "            \n",
    "            \n",
    "        \n",
    "    def _get_train_data_iter(self):\n",
    "        if self.train: # training data iteratot\n",
    "            iterator_train, types, shapes = data_iterator(self.dataset_path+'/text2ImgData.pkl',\n",
    "                                                          self.hparas['BATCH_SIZE'], training_data_generator)\n",
    "            iter_initializer = iterator_train.initializer\n",
    "            self.next_element = iterator_train.get_next()\n",
    "            self.sess.run(iterator_train.initializer)\n",
    "            self.iterator_train = iterator_train\n",
    "        else: # testing data iterator\n",
    "            iterator_test, types, shapes = data_iterator_test(self.dataset_path+'/testData.pkl', self.hparas['BATCH_SIZE'])\n",
    "            iter_initializer = iterator_test.initializer\n",
    "            self.next_element = iterator_test.get_next()\n",
    "            self.sess.run(iterator_test.initializer)\n",
    "            self.iterator_test = iterator_test\n",
    "            \n",
    "    def _input_layer(self):\n",
    "        if self.train:\n",
    "            self.real_image = tf.placeholder('float32',\n",
    "                                              [self.hparas['BATCH_SIZE'], self.hparas['IMAGE_SIZE'][0],\n",
    "                                               self.hparas['IMAGE_SIZE'][1], self.hparas['IMAGE_SIZE'][2]],\n",
    "                                              name='real_image')\n",
    "            self.caption = tf.placeholder(dtype=tf.int64, shape=[self.hparas['BATCH_SIZE'], None], name='caption')\n",
    "            self.z_noise = tf.placeholder(tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['Z_DIM']], name='z_noise')\n",
    "        else:\n",
    "            self.caption = tf.placeholder(dtype=tf.int64, shape=[self.hparas['BATCH_SIZE'], None], name='caption')\n",
    "            self.z_noise = tf.placeholder(tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['Z_DIM']], name='z_noise')\n",
    "    \n",
    "    def _get_inference(self):\n",
    "        if self.train:\n",
    "            # GAN training\n",
    "            # encoding text\n",
    "            text_encoder = TextEncoder(self.caption, hparas = self.hparas, training_phase=True, reuse=False)\n",
    "            self.text_encoder = text_encoder\n",
    "            # generating image\n",
    "            generator = Generator(self.z_noise, text_encoder.outputs, training_phase=True,\n",
    "                                  hparas=self.hparas, reuse=False)\n",
    "            self.generator = generator\n",
    "            \n",
    "            # discriminize\n",
    "            # fake image\n",
    "            fake_discriminator = Discriminator(generator.outputs, text_encoder.outputs,\n",
    "                                               training_phase=True, hparas=self.hparas, reuse=False)\n",
    "            self.fake_discriminator = fake_discriminator\n",
    "            # real image\n",
    "            real_discriminator = Discriminator(self.real_image, text_encoder.outputs, training_phase=True,\n",
    "                                              hparas=self.hparas, reuse=True)\n",
    "            self.real_discriminator = real_discriminator\n",
    "            \n",
    "        else: # inference mode\n",
    "            \n",
    "            self.text_embed = TextEncoder(self.caption, hparas=self.hparas, training_phase=False, reuse=False)\n",
    "            self.generate_image_net = Generator(self.z_noise, self.text_embed.outputs, training_phase=False,\n",
    "                                                hparas=self.hparas, reuse=False)\n",
    "    def _get_loss(self):\n",
    "        if self.train:\n",
    "            d_loss1 =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_discriminator.logits,\n",
    "                                                                              labels=tf.ones_like(self.real_discriminator.logits),\n",
    "                                                                              name='d_loss1'))\n",
    "            d_loss2 =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_discriminator.logits,\n",
    "                                                                              labels=tf.zeros_like(self.fake_discriminator.logits),\n",
    "                                                                              name='d_loss2'))\n",
    "            self.d_loss = d_loss1 + d_loss2\n",
    "            self.g_loss =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_discriminator.logits,\n",
    "                                                                                  labels=tf.ones_like(self.fake_discriminator.logits),\n",
    "                                                                                  name='g_loss'))\n",
    "    \n",
    "    def _optimize(self):\n",
    "        if self.train:\n",
    "            with tf.variable_scope('learning_rate'):\n",
    "                self.lr_var = tf.Variable(self.hparas['LR'], trainable=False)\n",
    "\n",
    "            discriminator_optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            generator_optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            self.d_optim = discriminator_optimizer.minimize(self.d_loss, var_list=self.discrim_vars)\n",
    "            self.g_optim = generator_optimizer.minimize(self.g_loss, var_list=self.generator_vars+self.text_encoder_vars)\n",
    "        \n",
    "    def training(self):\n",
    "        \n",
    "        for _epoch in range(self.hparas['N_EPOCH']):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            n_batch_epoch = int(self.hparas['N_SAMPLE']/self.hparas['BATCH_SIZE'])\n",
    "            for _step in range(n_batch_epoch):\n",
    "                step_time = time.time()\n",
    "                image_batch, caption_batch = self.sess.run(self.next_element)\n",
    "                b_z = np.random.normal(loc=0.0, scale=1.0, \n",
    "                                       size=(self.hparas['BATCH_SIZE'], self.hparas['Z_DIM'])).astype(np.float32)\n",
    "\n",
    "                # update discriminator\n",
    "                self.discriminator_error, _ = self.sess.run([self.d_loss, self.d_optim],\n",
    "                                                           feed_dict={\n",
    "                                                                self.real_image:image_batch,\n",
    "                                                                self.caption:caption_batch,\n",
    "                                                                self.z_noise:b_z})\n",
    "\n",
    "                # update generate\n",
    "                self.generator_error, _ = self.sess.run([self.g_loss, self.g_optim],\n",
    "                                                       feed_dict={self.caption: caption_batch, self.z_noise : b_z})\n",
    "                if _step%50==0:\n",
    "                    print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, d_loss: %.3f, g_loss: %.3f\" \\\n",
    "                            % (_epoch, self.hparas['N_EPOCH'], _step, n_batch_epoch, time.time() - step_time,\n",
    "                               self.discriminator_error, self.generator_error))\n",
    "            if _epoch != 0 and (_epoch+1)%5==0:\n",
    "                self._save_checkpoint(_epoch)\n",
    "                self._sample_visiualize(_epoch)\n",
    "            \n",
    "    def inference(self):\n",
    "        for _iters in range(100):\n",
    "            caption, idx = self.sess.run(self.next_element)\n",
    "            z_seed = np.random.normal(loc=0.0, scale=1.0, size=(self.hparas['BATCH_SIZE'], self.hparas['Z_DIM'])).astype(np.float32)\n",
    "\n",
    "            img_gen, rnn_out = self.sess.run([self.generate_image_net.outputs, self.text_embed.outputs],\n",
    "                                             feed_dict={self.caption : caption, self.z_noise : z_seed})\n",
    "            for i in range(self.hparas['BATCH_SIZE']):\n",
    "                scipy.misc.imsave(self.inference_path+'/inference_{:04d}.png'.format(idx[i]), img_gen[i])\n",
    "                \n",
    "    def _init_vars(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _get_session(self):\n",
    "        self.sess = tf.Session()\n",
    "    \n",
    "    def _get_saver(self):\n",
    "        if self.train:\n",
    "            self.rnn_saver = tf.train.Saver(var_list=self.text_encoder_vars)\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "            self.d_saver = tf.train.Saver(var_list=self.discrim_vars)\n",
    "        else:\n",
    "            self.rnn_saver = tf.train.Saver(var_list=self.text_encoder_vars)\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "            \n",
    "    def _sample_visiualize(self, epoch):\n",
    "        ni = int(np.ceil(np.sqrt(self.hparas['BATCH_SIZE'])))\n",
    "        sample_size = self.hparas['BATCH_SIZE']\n",
    "        max_len = self.hparas['MAX_SEQ_LENGTH']\n",
    "        \n",
    "        sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, self.hparas['Z_DIM'])).astype(np.float32)\n",
    "        sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"]*int(sample_size/ni) + [\"this flower has petals that are yellow, white and purple and has dark lines\"]*int(sample_size/ni) + [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) + [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "        for i, sent in enumerate(sample_sentence):\n",
    "            sample_sentence[i] = sent2IdList(sent, max_len)\n",
    "            \n",
    "        img_gen, rnn_out = self.sess.run([self.generator.outputs, self.text_encoder.outputs],\n",
    "                                         feed_dict={self.caption : sample_sentence, self.z_noise : sample_seed})\n",
    "        save_images(img_gen, [ni, ni], self.sample_path+'/train_{:02d}.png'.format(epoch))\n",
    "        \n",
    "    def _get_var_with_name(self):\n",
    "        t_vars = tf.trainable_variables()\n",
    "\n",
    "        self.text_encoder_vars = [var for var in t_vars if 'rnn' in var.name]\n",
    "        self.generator_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        self.discrim_vars = [var for var in t_vars if 'discrim' in var.name]\n",
    "    \n",
    "    def _load_checkpoint(self, recover):\n",
    "        if self.train:\n",
    "            self.rnn_saver.restore(self.sess, self.ckpt_path+'rnn_model_'+str(recover)+'.ckpt')\n",
    "            self.g_saver.restore(self.sess, self.ckpt_path+'g_model_'+str(recover)+'.ckpt')\n",
    "            self.d_saver.restore(self.sess, self.ckpt_path+'d_model_'+str(recover)+'.ckpt')\n",
    "        else:\n",
    "            self.rnn_saver.restore(self.sess, self.ckpt_path+'rnn_model_'+str(recover)+'.ckpt')\n",
    "            self.g_saver.restore(self.sess, self.ckpt_path+'g_model_'+str(recover)+'.ckpt')\n",
    "        print('-----success restored checkpoint--------')\n",
    "    \n",
    "    def _save_checkpoint(self, epoch):\n",
    "        self.rnn_saver.save(self.sess, self.ckpt_path+'rnn_model_'+str(epoch)+'.ckpt')\n",
    "        self.g_saver.save(self.sess, self.ckpt_path+'g_model_'+str(epoch)+'.ckpt')\n",
    "        self.d_saver.save(self.sess, self.ckpt_path+'d_model_'+str(epoch)+'.ckpt')\n",
    "        print('-----success saved checkpoint--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "checkpoint_path = './checkpoint/'\n",
    "inference_path = './inference'\n",
    "gan = GAN(get_hparas(), training_phase=True, dataset_path=data_path, ckpt_path=checkpoint_path, inference_path=inference_path)\n",
    "gan.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
